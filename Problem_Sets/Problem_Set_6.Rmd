---
title: 'GSE 520: Problem Set #6'
author: 
  - "Rus Adamovics-Davtian"
  - "Nick Brown"
  - "Amy Ozee"
output: html_document
---

### Question 1

Many times before estimating a linear regression model the analyst may perform some transformation on the data.  For example, if one of the variables is in dollars, they may divide it by 1000, so that it is reported in units of 1,000's of dollars.  Consider $X$ the original matrix of data (including a constant) and assume that all of the assumptions of the linear regression model are satisfied, such that 
$$Y = X \beta + \pmb \varepsilon$$
Where the OLS estimate of $\beta$ is $b = (X'X)^{-1}X'Y$

Next let $\text{T}$ be any transformation matrix that is $K \times K$ and is invertible.  
Define $X^* = X \text{T}$, which produces a linear transformation on the data.  The OLS estimate of the model with the transformed data is $b^* = (X^{*'} X^{*})^{-1} X^{*'} Y$

a. Plug in the definition of $X^*$ to the equation for $b^*$ to show that $b^* = \text{T}^{-1} b$ for any transformation matrix.

>**Solution:**  
\begin{align*}
b^* = & (X^{*'} X^{*})^{-1} X^{*'} Y\\
b^* = & [(XT)^{'}(XT)]^{-1} (XT)^{'} Y\\
b^* = & [(T^{'}X^{'})(XT)]^{-1} T^{'}X^{'} Y\\
b^* = & (XT)^{-1}(T^{'}X^{'})^{-1} T^{'}X^{'} Y\\
b^* = & T^{-1}[X^{-1}(X^{'})^{-1}][(T^{'})^{-1} T^{'}]X^{'}Y\\
b^* = & T^{-1} (X^{'}X)^{-1}IX^{'}Y\\
b^* = & T^{-1} (X^{'}X)^{-1}X^{'}Y\\
b^* = & T^{-1}b
\end{align*}

b. Use the result above to show that $\widehat{Y}$ the predicted values from the original regression equals $\widehat{Y}^*$ the predicted values from the regression with the transformed data.

>**Solution:**
\begin{align*}
\widehat{Y} = & \widehat{Y}^*\\
X(X^{'}X)^{-1}X^{'}Y = & XT(XT^{'}XT)^{-1}XT^{'}Y\\
Xb = & XTb^{*}\\
Xb = & XTT^{-1}b \quad \quad \quad since \quad b^{*} = T^{-1}b\\
Xb = & X(TT^{-1})b\\
Xb = & XIb\\
Xb = & Xb\\
P_xY = & P_xY
\end{align*}

c. Finally conclude by showing that given your findings above, the two regressions have the same predicted values, they will also have the same R-squared, that is you are proving the very important and intuitive result that any linear transformation of the data does not lead to an improved fit of the outcome variable. (note this result applies to linear transformations, i.e., non-linear transformations, like include log and squared terms may be very effective in increasing R-squared.)

>**Solution:**  
\begin{align*}
R^{2} = & 1 - \frac{SSE}{SST}\\
R^2 = & 1 - \frac{\sum_{i=1}^n (y_i - \hat y_i)}{\sum_{i=1}^n (y_i - \bar y)}\\
R^2 = & 1 - \frac{Y - \widehat Y}{Y - \overline Y}\\
\end{align*}

>Also from part b, $\widehat Y$ = $\widehat Y^{*}$ is equal to $P_xY$ = $P_xY$. Therefore,
\begin{align*}
1 - \frac{Y - \widehat Y}{Y - \overline Y} = & 1 - \frac{Y - \widehat Y^{*}}{Y - \overline Y}\\
1 - \frac{XB + \varepsilon - P_xY}{XB + \varepsilon - \overline Y} = & 1 - \frac{XB + \varepsilon - P_xY}{XB + \varepsilon - \overline Y}
\end{align*}


### Question 2

a.  Suppose we have the linear regression $y_i =  \beta_1 + \beta_2 x_i + \varepsilon_i$ that satisfies the assumptions of the classical regression model.  Define an estimator for $\beta_2$, 
$$b_2 = (y_2 - y_1) /(x_2 - x_1)$$
Where $y_1$ is the first observation in the sample with associated $x_1$ and and $y_2$ is the second observation with associated $x_2$.  Is this estimator for $\beta_2$ unbiased? Prove it by plugging in the value for $y_i$'s an doing algebra and taking expectations.

>**Solution:**  
First, define $b_2$ in terms of $\beta_2$ plus some error.
\begin{align*}
b_2 = & \frac{y_2 - y_1}{x_2 - x_1}\\
b_2 = & \frac{(\beta_1 + \beta_2x_2 + \varepsilon_2) - (\beta_1 + \beta_2x_1 + \varepsilon_1)}{x_2 - x_1}\\
b_2 = & \frac{\beta_2(x_2 - x_1) + (\varepsilon_2 - \varepsilon_1)}{x_2 - x_1}\\
b_2 =  & \frac{\beta_2(x_2 - x_1)}{x_2 - x_1} + \frac{\varepsilon_2 - \varepsilon_1}{x_2 - x_1}\\
b_2 = & \beta_2 + \frac{\varepsilon_2 - \varepsilon_1}{x_2 - x_1}\\
\end{align*}

> To show $b_2$ is unbiased, need to show $E[b_2] = \beta_2$. Can do that by finding $E[b_2 | X]$ and using Law of Expected Iteration to find $E[b_2]$. 
\begin{align*}
E[b_2 | X] = & E[\frac{(\beta_1 + \beta_2x_2 + \varepsilon_2) - (\beta_1 + \beta_2x_1 + \varepsilon_1)}{x_2 - x_1} | X]\\
E[b_2 | X] = & E[\beta_2 | X] + E[\frac{\varepsilon_2 - \varepsilon_1}{x_2 - x_1}]
\end{align*}

>Since $y_i$ follows all the classical regression assumptions, we know by assumption 3 that $E[\varepsilon | X] = 0$. Using that we are left with:

>
\begin{align*}
E[b_2 | X] = & E[\beta_2 | X] + E[\frac{0 - 0}{x_2 - x_1}]\\
E[b_2 | X] = & E[\beta_2 | X]\\
E[b_2 | X] = & \beta_2
\end{align*}

> Using Law of Expected Iteration...
\begin{align*}
E[b_2] = & E_x[E[b_2 | X]]\\
E[b_2] = & E_x[\beta_2] = \beta_2
\end{align*}
Therefore, $b_2$ is an unbiased estimator for $\beta_2$ since $E[b_2] - \beta_2 = 0$.

b.  Consider the multiple regression model containing three independent variables
$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + u$$
Assume that all of the assumptions of the linear regression model are satisfied.  
You are interested in estimating the sum of the parameters on $x_1$ and $x_2$; call this $\theta = \beta_1 + \beta_2$.  Show that $\hat{\theta} = \hat{\beta}_1 + \hat{\beta}_2$ is an unbiased estimator of $\theta$ where $\hat{\beta}_1$ and $\hat{\beta}_2$ are the OLS estimates of a regression of $x_1$, $x_2$, and $x_3$ on $y$.

>**Solution:**  
To show that $\hat \theta = \hat \beta_1 + \hat \beta_2$ is unbiased, show $E[\hat \theta] - \theta = 0$. Using Law of Expectation and Law of Expected Iteration...

>
\begin{align*}
E[\hat \theta] = & E[\hat \beta_1 + \hat \beta_2]\\
E[\hat \theta] = & E[\hat \beta_1] + E[\hat \beta_2]\\
E[\hat \theta] = & E_x[E[\hat \beta_1 | X]] + E_x[E[\hat \beta_2 | X]]
\end{align*}

> Need to solve for $E[\hat \beta_1 | X]$ and $E[\hat \beta_2 | X]$. Also, $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + u$ can be represented as $Y = X\beta + \mu$.
\begin{align*}
E[\hat \beta_1 | X] = & E[(X^{'}X)^{-1}X^{'}Y]\\
E[\hat \beta_1 | X] = & E[(X^{'}X)^{-1}X^{'}(X\beta_1 + \mu)]\\
E[\hat \beta_1 | X] = & E[(X^{'}X)^{-1}X^{'}X\beta_1 | X] + E[(X^{'}X)^{-1}X^{'}\mu | X]\\
E[\hat \beta_1 | X] = & E[\beta_1 | X] + (X^{'}X)^{-1}X^{'}E[\mu | X] \quad \quad \quad \quad \quad *E[\mu | X] = 0 \quad by \quad ASM \quad 3\\
E[\hat \beta_1 | X] = & E[\beta_1 | X]\\
E[\hat \beta_1 | X] = & \beta_1\\
\\
E[\hat \beta_2 | X] = & E[(X^{'}X)^{-1}X^{'}Y]\\
E[\hat \beta_2 | X] = & E[(X^{'}X)^{-1}X^{'}(X\beta_2 + \mu)]\\
E[\hat \beta_2 | X] = & E[(X^{'}X)^{-1}X^{'}X\beta_2 | X] + E[(X^{'}X)^{-1}X^{'}\mu | X]\\
E[\hat \beta_2 | X] = & E[\beta_2 | X] + (X^{'}X)^{-1}X^{'}E[\mu | X] \quad \quad \quad \quad \quad *E[\mu | X] = 0 \quad by \quad ASM \quad 3\\
E[\hat \beta_2 | X] = & E[\beta_2 | X]\\
E[\hat \beta_2 | X] = & \beta_2
\end{align*}

> Therefore, 
\begin{align*}
E[\hat \theta] = & E_x[E[\hat \beta_1 | X]] + E_x[E[\hat \beta_2 | X]]\\
E[\hat \theta] = & E_x[\beta_1] + E_x[\beta_2]\\
E[\hat \theta] = & \beta_1 + \beta_2\\
\\
Bias = & E[\hat \theta] - \theta = 0\\
Bias = & (\beta_1 + \beta_2) - (\beta_1 + \beta_2) = 0\\
\end{align*}
> $\hat{\theta} = \hat{\beta}_1 + \hat{\beta}_2$ is an unbiased estimator of $\theta$

### Question 3

Suppose R\&D for firms in the chemical industry is only a function of sales and satisfies the assumption of the linear regression model and can be written as
\begin{align*}
rd_i = \beta_1 + \beta_2 sales_i + \beta_3 sales^2_i + \varepsilon_i
\end{align*}
Where $E(\varepsilon | sales, sales^2) = E(\varepsilon ) = 0$

a. Consider a model without the squared term, with $rd_i = \beta_1 + \beta_2 sales_i + \varepsilon_i^*$, where $\varepsilon_i^* = \beta_3 sales^2_i + \varepsilon_i$.  Show by taking the conditional expectation of $\varepsilon_i^*$ with respect to $sales$ that this model violates the assumption of mean independence when $\beta_3 \neq 0$.

>**Solution:** 
\begin{align*}
E[\varepsilon_i^{*} | sales] = & E[\beta_3 sales^2_i + \varepsilon_i | sales]\\
E[\varepsilon_i^{*} | sales] = & E[\beta_3 sales^2_i | sales] + E[\varepsilon_i | sales] \quad \quad \quad \quad *By \quad ASM \quad 3, \quad E[\varepsilon_i | sales] = 0\\
E[\varepsilon_i^{*} | sales] = & E[\beta_3 sales^2_i | sales]\\
E[\varepsilon_i^{*} | sales] = & \beta_3
\end{align*}

b. Suppose we have the following estimates form the true model
$$\widehat{rd} = 17.79 -327.48(sales) +261.26(sales^2)$$
Let $\widetilde{b}_2$ be the estimate of the coefficient on sales when the squared term is NOT included in the regression.  Would $\widetilde{b}_2$ be larger or smaller than $-327$? Explain.

>**Solution:** 
By removing $sales^{2}$ in the regression equation and just focusing on how R&D changes with changes in sales, we would expect a positive coefficient to reflect the relationship between sales and R&D. Therefore, $\widetilde{b}_2$ will be larger than $-327$. The coefficient on sales is negative when $sales^{2}$ is included because $sales^{2}$ grows faster, quicker as the number of sales increases. The large, negative coefficient on sales is canceled out by the large, positive coefficient of $sales^{2}$.

### Question 4

The dataset ```voucher.RData``` contains data on elementary school students in 1994. For this analysis, remove all student who have a "NA" for the variables $mnce90$. 

A portion of these students  where awarded school vouchers to attend a charter, private or public school of their choosing.  The remaining students attended their local public school.  The variable $mnce_i$ is a math test for student $i$ and $select_i$ equals one if the student was selected to receive a voucher and zero otherwise.  

a. What is the mean and standard deviation of the math test score variables?

>**Solution:**
>```{r}
voucher <- load("C:/Users/rusla/OneDrive/GSE_520/R Data/voucher.RData")
data <- data[!is.na(data$mnce90),]
mean_math_90 <- mean(data$mnce90)
mean_math_94 <- mean(data$mnce)
sd_math_90 <- sd(data$mnce90)
sd_math_94 <- sd(data$mnce)
cat(" Mean Math Test Score 1990: ", round(mean_math_90,3), "\n",
    "SD Math Test Score 1990: ", round(sd_math_90,3), "\n", "\n",
    "Mean math Test Score 1994: ", round(mean_math_94,3), "\n", 
    "SD Math Test Score 1994: ", round(sd_math_94,3))
```

b. Standardize the test score variables by substracting the mean and dividing by the standard deviation.  Call this variables $mnceZ_i$. Estimate a simple linear regression model of the form:
$$ mnceZ_i = \beta_0 + \beta_1 select_i + \varepsilon_i$$
Report your results in equation form. According to the results do students that received a voucher do better or worse on the math test relative to those students that did not receive a voucher?  By how many standard deviations in test score units do voucher students perform better or worse?

>**Solution:**  
>```{r}
data$mnceZ <- (data$mnce - mean(data$mnce)) / sd(data$mnce)
model1 <- lm(mnceZ ~ select, data = data)
summary(model1)
```
$$\widehat{mnceZ_i} = 0.0224 - 0.0705(select_i)$$
> Students who received a voucher typically performed worse on the math test in 1994 (about 0.07 standard deviations lower) on average than students who did not receive a voucher.

c. Does your analysis in part b suggest that students that use vouchers cause math test scores to be lower?  Explain.

>**Solution:**  
Assuming the first three assumptions of the linear regression model are met, then yes. Furtheremore, one of the first assumptions is mean independence where $E[\varepsilon | X] = 0$. In other words, knowing the independent variables included in the model does not explain anything about the error. However, using just select to explain math test scores yields an $R^{2}$ value of only 0.001 and an $Adj R^{2}$ value that is negative. The model is not a good fit and explains virtually no variation in math test scores. Therefore, there must be some other important explanatory variables captured in the error term and not used in the equation. This means that $E[\varepsilon | X] \neq 0$ and we do not have a causal effect. 

d. It is possible that vouchers are not randomly assigned to students.  Consider three sets of student variables: gender, race/ethnicity, and past academic performance.  The variable $female$ is an indicator if the student is female.  The variables $hispanic$ and $black$ are indicators for the race/ethnicity of the student.  The omitted race/ethnic group is non-black/non-hispanic.  Finally the variables $mnce90$ is a math score for the student 4 years earlier, which is a measure of past academic performance.
<br/><br/>
Run a regression with the variable $select$ as the dependent variable and the variables listed above as explanatory variables.  Report your results in equation form.  Do these results suggest the vouchers are randomly assigned or do the students that receive vouchers vary systematically from the students that do not receive the vouchers?  Use the results from this regression to characterize the main difference between students that received these vouchers versus those that did not, e.g., those likely to receive vouchers are less likely to be female, etc. 

>**Solution:**  
>```{r}
model2 <- lm(select ~ female + hispanic + black + mnce90, data = data)
summary(model2)
```
$$\widehat{select_i} = 0.0995 + 0.0336(female_i) + 0.4604(hispanic_i) + 0.2766(black_i) - 0.0011(mnce90_i)$$

> From the regression output, we expect students who are black or hispanic to be more likely to have received a voucher than those who are not black or hispanic. Also, we expect females to be slightly more likely than males to have received a voucher. Lastly, there seems to be little to no effect of math test scores in 1990 on the voucher selection process. In summary, there seems to be enough evidence to suggest that vouchers are not randomly assigned to students. There seems to be some evidence that the selection process depends on other factors as well. However, the $R^{2}$ and $AdjR^{2}$ values are still very low (~ 0.08) so there may be some additional important factors left out that also influence the selection process.      

e. Re-do the analysis in part (b) but now add control variables mentioned in part (d).  Report your results in equation form.  According to the results do students that received a voucher do better or worse on the math test relative to those students that did not receive a voucher once we control for these additional variables?  By how many standard deviations in test score units do voucher students perform better or worse?

>**Solution:**  
>```{r}
model3 <- lm(mnceZ ~ select + female + hispanic + black + mnce90, data = data)
summary(model3)
```
$$\widehat{mnceZ_i} = -0.9977 + 0.1415(select_i) - 0.0450(female_i) - 0.2484(hispanic_i) - 0.4248(black_i) + 0.0296(mnce90_i)$$

> Holding all other variables constant, students who received a voucher typically performed better on the math test in 1994 (about 0.14 standard deviations higher) on average than students who did not receive a voucher.

f. Which results are more likely to represent the causal effect of vouchers on test scores, the results in part (b) or the results from part (e)?  Why?

>**Solution:** 
The results in part (e) are more likely to represent the causal effect of vouchers on test scores. The model in part (e) explains more variation in math test scores by including more significant eplanatory variables than the model in part (b). This can be seen from the substantial increases in $R^{2}$ and $AdjR^{2}$ values from the model in part (e) compared to those values for the model in part (b).

g.  Being asiable against all other explanatory variables and multiplying the effect of the var clear as possible explain how each of the variables, $female$, $black$, $hispanic$, and $mnce90$ contribute to the omitted variable bias when they are not included in the regression in part (b).  Which variable contributes the least, which contributes the most?

>**Solution:**
>```{r}
round(coef(model1),3)
round(coef(model2),3)
```
>The omitted variable bias can be computed by taking the difference in coefficients from the full model (including the omitted variable) to the simple model (without including the omitted variable). It can also be found by regressing the omitted variable of interest with the effect of the omitted variable in the full regression model. In this problem, we calculated coefficients of the omitted variables when used as explanatory variables for X (X being the select variable) in part (d). That regression gave us all non-zero coefficients for these variables. Thus, the only way these variables would not contribute to the omitted variable bias is if they had no effect on the dependent variable mnceZi. Since those coefficients in part (d) where all not zero, then all of those variables contributed to the bias for the model in part (b). The variable that contributes least to the omitted variable bias is mnce90 as it had the smallest coefficient (−0.001) from the regression we ran in part (d). The variable with the biggest contribution to omitted variable bias is hispanic as it had the largest coefficient (0.46) from the regression in part (d).  

h. Finally, assume you had access to a variable about parental income.  Explain under what scenarios the inclusion of this additional variable in the model from part (e), would make the coefficient on $select$ smaller than you found in part (e). 

>**Solution:**  
A scenario under which the inclusion of parental income would make the coefficient on select smaller would be if select is highly correlated with parental income. When regressing select against all other variables, the coefficient should be significantly different from zero and large. If the effect of parental income on math test scores can be explained better than from the effect of select, then the coefficient of select will be smaller than in part (e). This is due to parental income explaining part of the math test score variables that select was previousy explaining.

### Question 5

Consider the simple linear regression model, $y_i = \beta_1 + \beta_2 x_i + \varepsilon_i$ that satisfies mean independence $E(\varepsilon | x) = E(\varepsilon) = 0$.  

Suppose $x$ is not observed.  However, $x^*$ is observed which is defined as $x_i^* = x_i + u_i$, where $u$ is called measurement error.  Assume that $E(u)=0$ and that $E(u|x)=0$.  That is, $u$ does not depend on $x$.  This is called classical measurement error because it is purely random.  Since $x$ is not observed, consider the regression of $y$ on $x^*$:
$$y_i = \beta_1 + \beta_2 x_i^* + \varepsilon^*_i$$

a.  Write explicitly the determinants of $\varepsilon^*_i$.  You do this by plugging in the $x_i = x_i^* - u_i$ into the original equation and re-arrange terms.

>**Solution:**
\begin{align*}
y_i = \beta_1 + \beta_2 (x_i^* - u_i) + \varepsilon^*_i\\
y_i = \beta_1 + \beta_2 x_i^* - \beta_2 u_i + \varepsilon^*_i\\
\varepsilon^*_i = y_i - \beta_1 - \beta_2 x_i^* + \beta_2 u_i
\end{align*}

b. Show that $\varepsilon^*_i$ is not mean independent of $x_i^*$.

>**Solution:**
\begin{align*}
E[\varepsilon^*_i | x_i^{*}] = & E[y_i - \beta_1 - \beta_2 x_i^* + \beta_2 u_i | x_i^{*}]\\
E[\varepsilon^*_i | x_i^{*}]  = & E[y_i | x_i^{*}] - E[\beta_1 | x_i^{*}] - E[\beta_2 x_i^* | x_i^*] + E[\beta_2 u_i | x_i^*]\\
E[\varepsilon^*_i | x_i^{*}]  = & E[y_i | x_i^{*}] - \beta_1 - \beta_2 E[x_i^{*} | x_i^{*}] + \beta_2 E[u_i | x_i^*]\\
E[\varepsilon^*_i | x_i^{*}]  = & E[y_i | x_i^{*}] - \beta_1 - \beta_2 x_i^{*} + \beta_2 E[x_i^* - x_i | x_i^*]\\
E[\varepsilon^*_i | x_i^{*}]  = & E[y_i | x_i^{*}] - \beta_1 - \beta_2 x_i^{*} + \beta_2 (E[x_i^* | x_i^*] - E[x_i | x_i^*])\\
E[\varepsilon^*_i | x_i^{*}]  = & E[y_i | x_i^{*}] - \beta_1 - \beta_2 x_i^{*} + \beta_2 x_i^{*} - \beta_2 E[x_i | x_i^*]\\
E[\varepsilon^*_i | x_i^{*}]  = & E[y_i | x_i^{*}] - \beta_1 - \beta_2 E[x_i | x_i^*]\\
\end{align*}


c. Treating $u$ as an omitted variable, show that if you estimate  $y_i = \beta_1 + \beta_2 x_i^* + \varepsilon^*_i$ with OLS, the estimate of $\beta_2$ will always be biased towards zero, that is, if $\beta_2>0$, the bias will be negative and if $\beta_2<0$ the bias will be positive.  This is called attenuation bias because it always produces estimated effects that are always weakened (closer to zero).

**Solution:**
\begin{align*}
y_i = \beta_1 + \beta_2 x_i^* + \varepsilon^*_i\\
\widehat Y = X^*b
\end{align*}


