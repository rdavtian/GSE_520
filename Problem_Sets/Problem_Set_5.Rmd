---
title: 'GSE 520: Problem Set #5'
author: 
  - "Rus Adamovics-Davtian"
  - "Trevor Luenser"
  - "Taymour Siddiqui"
output: html_document
---

### Question 1

Given the data vector $y$ and the data matrix $X$ (which includes a column of ones).  Let $b$ be the least squares estimate of $\beta$ for the relationship $y = X \beta + \varepsilon$, where $\varepsilon$ is a vector of disturbances.  Prove the following relationships.

a. Define $e$ as the vector of residuals, such that $e = y - Xb$.  Show that $\sum_i e_i=0$. (Hint: rely on the first order condition of the SSE with respect to the intercept)

>**Solution:**
\begin{align*}
SSE = & \sum_{i=1}^n e_i^{2} = \sum_{i=1}^n(y_i - X_i^{T}b)^{2}\\
\frac{dSSE}{db_i} = & -2\sum_{i=1}^n x_i(y_i - X_i^{T}b) = 0 \quad x_{i} = 1 \: \forall i\\
0 = & \sum_{i=1}^n (1)(y_i - X_i^{T}b) = \sum_{i=1}^n e_i\\
\end{align*}

b. Define $\overline{x}$ as a $K \times 1$ vector which contains the mean for each column of $X$.  Let $\overline{y}$ be the mean of $y$.  Show that the result above also implies $\overline{y} = \overline{x}'b$. (Hint: rely on your results from part (a) to prove this)

>**Solution:**  
From part a...
\begin{align*}
0 = & \sum_{i=1}^n (y_i - X_i^{T}b)\\
0 = & \sum_{i=1}^n (y_i) - \sum_{i=1}^n(X_i^{T}b)\\
0 = & \overline{y} - \overline{x}^{T}b\\
\overline{y} = & \overline{x}^{T}b
\end{align*}

### Question 2

Suppose we would like to estimate the production function 
$$y_i = \beta_1x_i^{\beta_2}g_i^{\beta_3}h_i^{\beta_4}e^{\varepsilon_i}$$
Where $e$ is the exponential function.  Describe how the parameters of this function can be estimate using ordinary least squares under the restriction of constant returns to scale, (i.e. $\beta_2+\beta_3+\beta_4=1$).

>**Solution:**
If $\beta_2 + \beta_3 + \beta_4 = 1$, then $\beta_4 = 1 - \beta_2 - \beta_3$
\begin{align*}
y_i = & \beta_1x_i^{\beta_2}g_i^{\beta_3}h_i^{\beta_4}e^{\varepsilon_i}\\
\ln(y_i) = & \ln(\beta_1) + \beta_2\ln(x_i) + \beta_3\ln(g_i) + \beta_4\ln(h_i) + \varepsilon_i\\
\ln(y_i) = & \ln(\beta_1) + \beta_2\ln(x_i) + \beta_3\ln(g_i) + (1 - \beta_2 - \beta_3)\ln(h_i) + \varepsilon_i\\
\ln(y_i) = & \ln(\beta_1) + \beta_2\ln(x_i) + \beta_3\ln(g_i) + \ln(h_i) - \beta_2\ln(h_i) - \beta_3\ln(h_i) + \varepsilon_i\\
\ln(y_i) - \ln(h_i) = & \ln(\beta_1) + \beta_2[\ln(x_i) - \ln(h_i)] + \beta_3[\ln(g_i) - \ln(h_i)] + \varepsilon_i
\end{align*}

>
\begin{align*}
Where \quad y^* = \ln(y_i) - \ln(h_i), \quad x^*_1 = \ln(x_i) - \ln(h_i) \quad and \quad x^*_2 = \ln(g_i) - \ln(h_i).
\end{align*}

### Question 3

We are interested in estimating the influence of $x$ on $y$.  Consider the bivariate regression model 
$$y_i = \beta_0 + \beta_1 x_i + \varepsilon_i$$  
We assume that all of the assumptions of the linear regression model are satisfied.  We collect data for 4 individuals.  Define the matrix $X$ as a 4 by 2 matrix, where row $i$ is defined as $x'_i = [1 \quad x_i]$.   $Y$ is a column vector with $y_i$ as the $i^{th}$ element.  In the data we observe
\begin{align*}
(X'X) = \left[
\begin{matrix}
4 & 46 \\
46 & 534
\end{matrix} \right] 
\qquad 
(X'Y)= \left[
\begin{matrix}
12.7 \\
146.4
\end{matrix} \right] 
\end{align*}
Use these values to estimate $\beta_0$ and $\beta_1$.

>**Solution:**  
>```{r}
x_t_x <- matrix(c(4, 46, 46, 534), ncol = 2, nrow = 2)
x_t_y <- matrix(c(12.7, 146.4), ncol = 1, nrow = 2)
betas <- (solve(x_t_x) %*% x_t_y)
cat(" b0 = ", betas[1],"\n",
    "b1 = ", betas[2])
```

### Question 4

In Problem Set 2, Question 6(c) you were asked to interpret a demand equation for organic apples.  This equation was estimated using OLS with the dataset ```apple.RData```.  Take this data and reproduce these numbers using the matrix formula $b = (X'X)^{-1}X'Y$ (you must create the corresponding $X$ and $Y$ matrices and apply the formula).   Confirm your results by using \textsf{R}'s built in \textsf{lm} function.

>**Solution:**  
```{r, warning = FALSE}
apples <- load("C:/Users/rusla/OneDrive/GSE_520/R Data/apple.RData")
data$educ_ecoprc <- data$educ*data$ecoprc
X <- as.matrix(data[, c("educ","regprc","ecoprc","educ_ecoprc")])
X <- as.matrix(cbind(rep(1, nrow(data)),X))
Y <- as.matrix(data[, "ecolbs"])
x_t_x <- t(X) %*% X
x_t_y <- t(X) %*% Y
betas <- round((solve(x_t_x) %*% x_t_y),3)
paste0("Beta Coefficients using the Matrix Formula")
cat(" intercept = ", betas[1],"\n",
   "educ = ", betas[2],"\n",
   "regprc = ", betas[3],"\n",
   "ecoprc = ", betas[4],"\n",
   "educ*ecoprc = ", betas[5])
paste0("Beta Coefficients using lm function")
round(coef(lm(ecolbs ~ educ + regprc + ecoprc + educ_ecoprc,data = data)),3)
```

### Question 5

Let $X$ be a $n \times K$ matrix that includes a column of ones.  Let $Z$ also be a matrix formed by concatenating the column vectors $Z = \left[ z_1 \quad z_2 \quad z_3 \right]$.  In words, explain what results from $PZ$, where $P$ is the projection matrix of $X$, making sure to explain the resulting dimension and provide the interpretation of the values.

>**Solution:**  
\begin{align*}
X = & (n*k) \quad matrix\\
X^{T} = & (k*n) \quad matrix\\
(X^{T}X)^{-1} = & (k*k) \quad matrix\\
Z = & (n*k) \quad matrix
\end{align*}

>Therefore,
\begin{align*}
PZ = & X(X^{T}X)^{-1}X^{T}Z = (n*k) \quad matrix
\end{align*}
This projection matrix P creates predicted values for the column vectors of Z (3 column vectors with n rows) with linear combinations of columns in X.

### Question 6

While I was watching ESPN classic last night I collected data on the number of points scored for 5 players.  The observed points where 
\begin{align*}
\left[\begin{matrix} 11 & 11 & 15 & 7 & 26 \end{matrix} \right]
\end{align*}
Next I was interested in seeing if the individual's draft order could predict points.  Lower draft numbers should indicate better players.

I estimated the equation
$$point_i = \beta_1 + \beta_2 draft_i + \varepsilon_i$$
where $draft_i$ was the draft number of player $i$.

I got the following estimates and R-squared from my regression
\begin{align*}
\widehat{points} = & 18.894 - 0.207(draft) \\
\text{R-Squared: } & 0.33
\end{align*}

a. What was the average draft number of the 5 players I was watching?

>**Solution:**
```{r}
points <- c(11,11,15,7,26)
drafted <- (18.894 - points) / 0.207
paste0("Based off the points observed and estimated regression equation, the average draft order predicted for          these five players is ",round(mean(drafted),3))
```


b. What was my adjusted R-squared?

>**Solution:**
```{r}
n <- length(points)
k <- 1
SST = sum((points - mean(points))^2)
SSE = SST*(1 - 0.33)
R2_adj <- 1 - ((SSE / (n - k)) / (SST / (n - 1)))
paste0("Adjusted R^2 = ", R2_adj)
```

### Question 7

Pick one of the datasets in the data folder that sounds interesting.  Pick a variable as your dependent variables and pick 3 or 4 explanatory variables.  

a. What data set did you pick and what did you pick for your dependent variables and explanatory variable?

>**Solution:**
We chose the mlb1 dataset and wanted to look at what performance metrics can help explain a players salary. The dependent variable is log of salary and we chose games played per year, runs scored per year, RBI's per year, and career hits as explanatory variables. These variables were chosen based off high linear correlations to log salary.

```{r}
mlb1 <- load("C:/Users/rusla/OneDrive/GSE_520/R Data/mlb1.RData")
head(round(sort(cor(data)[,"lsalary"], decreasing = T),3),15)
```

b. Estimate a linear regression model with these variables and report your results in equation form along with the R-squared. Use a format similar to how I presented the estimation results for question 6 above.

>**Solution:**
The estimated regression equation is of the following...  
\begin{align*}
lsalary_i = & \beta_0 + \beta_1(gamesyr) +  \beta_2(runsyr) + \beta_3(rbisyr) + \beta_4(hits)\\
\end{align*}
Using matrix algebra...
```{r}
X <- as.matrix(data[, c("gamesyr","runsyr","rbisyr","hits")])
X <- as.matrix(cbind(rep(1, nrow(data)),X))
Y <- as.matrix(data[, "lsalary"])
x_t_x <- t(X) %*% X
x_t_y <- t(X) %*% Y
betas <- round((solve(x_t_x) %*% x_t_y),5)
cat(" intercept = ", betas[1],"\n",
   "gamesyr = ", betas[2],"\n",
   "runsyr = ", betas[3],"\n",
   "rbisyr = ", betas[4],"\n",
   "hits = ", betas[5])
```
\begin{align*}
lsalary_i = & 11.733 + 0.00884(gamesyr) + 0.00937(runsyr) + 0.01164(rbisyr) + 0.00033(hits)
\end{align*}

c. Comment on your results.  Quantiatively describe some of the marginal effects, i.e., "if $x$ increases by 1 unit then $y$ increases by ...".  Do all of the marginal effects have an intuitive sign?  Are any of the marginal effects surprisingly large? Are any of the marignal effect economically meaningless?

>**Solution:**  
For each additional 10 RBIs per season holding all other variables constant, we expect a players salary to increase by 11.64% [(100 * $\beta_3$) * 10]. For each additional 10 runs scored per season holding all other variables constant, we expect a players salary to increase by 9.37% [(100 * $\beta_2$) * 10]. For each additional 10 games played per season holding all other variables constant, we expect a players salary to increase by 8.84% [(100 * $\beta_2$) * 10].

> All marginal effects have intuitive signs as better performance should yield higher salaries. However, the marginal effect of hits on percent of log salary is very small. This is due to looking at hits over a span of a players' career instead of per season. Any small change in career hits will yield almost meaningless changes in salary. 

### Question 8

Repeat Question 7 but with a different dataset.

>**Solution:**
We chose the RENTAL dataset and wanted to look at what factors can help explain household income. The dependent variable is annual total household income (FTOTINC) and we chose gross yearly rent (RENTGRS), number of auto thefts per 100,000 (AUTO), and number of total rooms (ROOMS) as explanatory variables. These variables were chosen based off high linear correlations to household income.
```{r}
rental <- load("C:/Users/rusla/OneDrive/GSE_520/R Data/Rental.RData")
head(round(sort(cor(data)[,"FTOTINC"], decreasing = T),3),20)
```

>**Solution:**
The estimated regression equation is of the following...  
\begin{align*}
FTOTINC = & \beta_0 + \beta_1(RENTGRS) +  \beta_2(AUTO) + \beta_3(ROOMS)
\end{align*}
Using matrix algebra...
```{r}
X <- as.matrix(data[, c("RENTGRS","AUTO","ROOMS")])
X <- as.matrix(cbind(rep(1, nrow(data)),X))
Y <- as.matrix(data[, "FTOTINC"])
x_t_x <- t(X) %*% X
x_t_y <- t(X) %*% Y
betas <- round((solve(x_t_x) %*% x_t_y),5)
cat(" intercept = ", betas[1],"\n",
   "RENTGRS = ", betas[2],"\n",
   "AUTO = ", betas[3],"\n",
   "ROOMS = ", betas[4])
```
\begin{align*}
RENTGRS = & -1822.097 + 36.5813(RENTGRS) + 3.1520(AUTO) + 1360.037(ROOMS)
\end{align*}

>**Solution:**  
For each additional dollar increase in yearly rent holding all other variables constant, we expect the annual total household income to increase by $36.58. A one unit increase in the number of rooms holding all other variables constant results in an increase of expected annual household income by $1360.04. For each additional auto theft per 100,000 people holding all other variables constant, we expect the annual total household income to increase by $3.15.

> We would expect the sign on number of auto thefts per 100,000 people to be negative since higher levels of crime usually occur in areas of lower socioeconomic status where the household income is lower.